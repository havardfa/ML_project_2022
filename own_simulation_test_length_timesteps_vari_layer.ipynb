{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1OL5uMOooPbDe2uhGJHzVpZdAY4mJAfsm",
      "authorship_tag": "ABX9TyPQk5DNCkauJLJIk6gsUqCt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/PhD/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Cqm1kTSCSOt",
        "outputId": "8e678661-46f1-4ff4-9ff3-1554b8919cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WinError 3] The system cannot find the path specified: 'drive/MyDrive/PhD/'\n",
            "C:\\Users\\haavamfa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "safc4MNTSNd1"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "#sys.path.append(r'C:\\Users\\haavamfa\\Documents\\extratrees\\Preprocessing')\n",
        "#sys.path.append(r'C:\\Users\\haavamfa\\Documents\\extratrees\\Machine_Learning')\n",
        "#sys.path.append(r'C:\\Users\\haavamfa\\Documents\\PhD\\Abaqus_Preprocessed')\n",
        "#sys.path.append('Machine_Learning')\n",
        "#sys.path.append(r'gdrive/MyDrive/Colab Notebooks/extratrees/Preprocessing')\n",
        "#%cd drive\\MyDrive\\Colab Notebooks\\extratrees\n",
        "import pandas as pd\n",
        "\n",
        "#import matplotlib.pyplot as plt\n",
        "#import seaborn as sns\n",
        "#from dataset import label_test_split,constant_removal,layer_split,data_split\n",
        "\n",
        "#from extra_trees import extra_tree_model,nodes_predict, feature_importance, permutation_feature_importance,performance,predicted_plot, extra_time\n",
        "#from correlation import correlation_heatmap, highly_correlated_features, shearman_correlation_heatmap,shearman_dendrogram\n",
        "#from sklearn.inspection import plot_partial_dependence\n",
        "#from sklearn.inspection import plot_partial_dependence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "filename1 =r'C:\\Users\\haavamfa\\Documents\\PhD\\Abaqus_Preprocessed\\with_length_and_time\\scripted_thermal_048_jan_smallerbase_7layers_fixed_compressed.csv'\n",
        "#filename1 = 'TestOutput_1layer_line.csv'\n",
        "data1 = pd.read_csv(filename1, header = 0, sep=',', index_col=False)\n",
        "data1=data1.sort_values(by=['t'], axis=0)\n",
        "data1=data1.sample(frac = 1)\n",
        "#print(data1)\n",
        "data1=data1.drop(columns = ['i','T_6','T_7','T_8','T_9','T_10', 'x', 'y', 'z', 'l_pos', 'delta_l', 'bar_length'], axis = 1)\n",
        "#data1=data1.drop(columns = ['i', 'y', 'z'], axis = 1)\n",
        "#print(data1)\n",
        "X_1 = data1.drop(columns = 'T', axis = 1)\n",
        "Y_1 = data1['T']\n",
        "#X_1 = data1.drop(columns = ['T', 'x', 'y', 'z'], axis = 1)\n",
        "#Y_1 = data1.drop(columns = ['t', 'T_1', 'T_2', 'T_3', 'T_4', 'T_5'], axis = 1)\n",
        "\n",
        "print(Y_1)\n",
        "\n",
        "filename2 =r'C:\\Users\\haavamfa\\Documents\\PhD\\Abaqus_Preprocessed\\with_length_and_time\\scripted_thermal_048_jan_smallerbase_8layers_fixed_compressed.csv'\n",
        "#filename1 = 'TestOutput_1layer_line.csv'\n",
        "data2 = pd.read_csv(filename2, header = 0, sep=',', index_col=False)\n",
        "data2=data2.sort_values(by=['t', 'x'], axis=0)\n",
        "data2=data2.sample(frac = 1)\n",
        "#print(data2)\n",
        "data2=data2.drop(columns = ['i','T_6','T_7','T_8','T_9','T_10','x', 'y', 'z', 'l_pos', 'delta_l', 'bar_length'], axis = 1)\n",
        "#print(data1)\n",
        "X_2 = data2.drop(columns = 'T', axis = 1)\n",
        "Y_2 = data2['T']\n",
        "\n",
        "filename3 =r'C:\\Users\\haavamfa\\Documents\\PhD\\Abaqus_Preprocessed\\with_length_and_time\\scripted_thermal_048_jan_smallerbase_9layers_fixed_compressed.csv'\n",
        "#filename1 = 'TestOutput_1layer_line.csv'\n",
        "data3 = pd.read_csv(filename3, header = 0, sep=',', index_col=False)\n",
        "data3=data3.sort_values(by=['t', 'x'], axis=0)\n",
        "data3=data3.sample(frac = 1)\n",
        "#print(data3)\n",
        "data3=data3.drop(columns = ['i','T_6','T_7','T_8','T_9','T_10','x', 'y', 'z', 'l_pos', 'delta_l', 'bar_length'], axis = 1)\n",
        "#print(data1)\n",
        "X_3 = data3.drop(columns = 'T', axis = 1)\n",
        "Y_3 = data3['T']\n",
        "\n",
        "filename4 =r'C:\\Users\\haavamfa\\Documents\\PhD\\Abaqus_Preprocessed\\with_length_and_time\\scripted_thermal_048_jan_smallerbase_10layers_fixed_compressed.csv'\n",
        "data4 = pd.read_csv(filename4, header = 0, sep=',', index_col=False)\n",
        "data4=data4.sort_values(by=['t', 'x'], axis=0)\n",
        "data4=data4.sample(frac = 1)\n",
        "#print(data4)\n",
        "data4=data4.drop(columns = ['i','T_6','T_7','T_8','T_9','T_10','x', 'y', 'z', 'l_pos', 'delta_l', 'bar_length'], axis = 1)\n",
        "#print(data1)\n",
        "X_4 = data4.drop(columns = 'T', axis = 1)\n",
        "Y_4 = data4['T']\n",
        "\n",
        "filename5 =r'C:\\Users\\haavamfa\\Documents\\PhD\\Abaqus_Preprocessed\\with_length_and_time\\scripted_thermal_048_jan_smallerbase_6layers_fixed_compressed.csv'\n",
        "#filename1 = 'TestOutput_1layer_line.csv'\n",
        "data5 = pd.read_csv(filename5, header = 0, sep=',', index_col=False)\n",
        "data5= data5.sort_values(by=['i'], axis = 0)\n",
        "data5=data5.sample(frac = 1)\n",
        "#print(data5)\n",
        "data5=data5.drop(columns = ['i','T_6','T_7','T_8','T_9','T_10','x', 'y', 'z', 'l_pos', 'delta_l', 'bar_length'], axis = 1)\n",
        "#print(data5)\n",
        "X_5 = data5.drop(columns = 'T', axis = 1)\n",
        "Y_5 = data5['T']\n",
        "\n",
        "#X = X_1.append(X_2)\n",
        "#Y = Y_1.append(Y_2)\n",
        "#X = X.append(X_3)\n",
        "#Y = Y.append(X_3)\n",
        "#X = X.append(X_4)\n",
        "#Y = Y.append(Y_4)\n",
        "#X = X.append(X_5)\n",
        "#Y = Y.append(Y_5)\n",
        "\n",
        "#X_scale = X.append(X_3)\n",
        "#X_scale = X.append(X_3)\n",
        "\n",
        "#X = X.sample(frac = 1)\n",
        "#Y = Y.sample(frac = 1)"
      ],
      "metadata": {
        "id": "HwYxhCPcSv9q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc1be975-a82c-470f-e182-90db6c4cf14d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7795964     255.281097\n",
            "9100917     376.944611\n",
            "11430979    360.735352\n",
            "2143332     262.820038\n",
            "8311768     353.814331\n",
            "               ...    \n",
            "7580719     199.545532\n",
            "9448765     244.302780\n",
            "9081468     482.409424\n",
            "2707855     301.082733\n",
            "4808688     301.762299\n",
            "Name: T, Length: 12713427, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = X_1.append(X_2)\n",
        "Y = Y_1.append(Y_2)\n",
        "X = X.append(X_3)\n",
        "Y = Y.append(X_3)\n",
        "X = X.append(X_4)\n",
        "Y = Y.append(Y_4)\n",
        "X = X.append(X_5)\n",
        "Y = Y.append(Y_5)\n",
        "\n",
        "X_scale = X.append(X_3)\n",
        "X_scale = X.append(X_3)"
      ],
      "metadata": {
        "id": "QsYwlY_G-x_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import csv\n",
        "#import sklearn\n",
        "#import tensorflow \n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "#import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "#from sklearn.model_selection import KFold\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
        "from torch.nn import functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets,transforms\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "oW5D3nbcfvOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, X, y, scale_data=True):\n",
        "    if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
        "      #if scale_data == True:\n",
        "        #X = StandardScaler().fit_transform(X)\n",
        "      self.X = torch.from_numpy(X)\n",
        "      self.y = torch.from_numpy(y)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.X)\n",
        "  \n",
        "  def __getitem__(self, i):\n",
        "      return self.X[i], self.y[i]\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    layerSize = 64\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(6, layerSize),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(layerSize, layerSize),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(layerSize, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "      Forward pass\n",
        "    '''\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "ufLTX76AfyO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#torch.manual_seed(42)\n",
        "criterion = nn.SmoothL1Loss()\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "#from sklearn.model_selection import KFold\n",
        "\n",
        "torch.use_deterministic_algorithms(True)"
      ],
      "metadata": {
        "id": "rjdkYnU1fzhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalization\n",
        "\n",
        "def minMax(inp, min, max):\n",
        "  scaled = (inp - min)/(max - min)\n",
        "  return scaled\n",
        "\n",
        "def minMaxInverse(inp, min, max):\n",
        "  unscaled = (inp *(max - min)) + min\n",
        "  return unscaled\n",
        "\n",
        "\n",
        "#For 4layers\n",
        "#min_x = np.array([0, 20, 20, 20, 20, 20])\n",
        "#max_x = np.array([538, 1790, 1790, 1790, 1790, 1790])\n",
        "\n",
        "#For varying layer amt\n",
        "min_x = np.array([0, 20, 20, 20, 20, 20])\n",
        "max_x = np.array([850, 1650, 1650, 1650, 1650, 1650])\n",
        "\n",
        "#For varying speed\n",
        "#min_x = np.array([0, 20, 20, 20, 20, 20])\n",
        "#max_x = np.array([474, 2080, 2080, 2080, 2080, 2080])\n",
        "\n",
        "#For 1 layer\n",
        "#min_x = np.array([0, 20, 20, 20, 20, 20])\n",
        "#max_x = np.array([450, 1520, 1520, 1520, 1520, 1520])\n",
        "\n",
        "#min_y = np.array([20])\n",
        "#max_y = np.array([1727])\n",
        "\n",
        "#test_numpy_y = minMax(Y_2.to_numpy(), min_y, max_y)\n",
        "#test_numpy_y = test_numpy_y.reshape(-1, 1)\n",
        "test_numpy_x = minMax(X_4.to_numpy(), min_x, max_x)\n",
        "test_numpy_y = Y_4.to_numpy()\n",
        "\n",
        "#print(max(test_numpy_y))\n",
        "\n",
        "#print(len(test_numpy_y))\n",
        "\n",
        "dataset = TestDataset(test_numpy_x, test_numpy_y)\n",
        "\n"
      ],
      "metadata": {
        "id": "OxveZQnub84a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardization\n",
        "\n",
        "#test_numpy_y = torch.from_numpy(test_numpy_y)\n",
        "#test_numpy_y = StandardScaler().fit_transform(test_numpy_y)\n",
        "#test_numpy_y = test_numpy_y.to_numpy()\n",
        "\n",
        "\n",
        "#test_numpy_x = torch.from_numpy(test_numpy_x)\n",
        "#test_numpy_x = StandardScaler().fit_transform(test_numpy_x)\n",
        "#test_numpy_x = test_numpy_x.to_numpy()\n",
        "\n",
        "test_numpy_x = X_1.to_numpy()\n",
        "test_numpy_y = Y_1.to_numpy()\n",
        "\n",
        "test_numpy_x_scale = X_5.to_numpy()\n",
        "#test_numpy_y_scale = Y_3.to_numpy()\n",
        "\n",
        "#print(test_numpy_x)\n",
        "\n",
        "means_x = test_numpy_x_scale.mean(0)\n",
        "deviations_x = test_numpy_x_scale.std(0)\n",
        "\n",
        "#means_y = test_numpy_y_scale.mean(0)\n",
        "#deviations_y = test_numpy_y_scale.std(0)\n",
        "\n",
        "#print(means)\n",
        "#print(deviations)\n",
        "\n",
        "test_numpy_x = (test_numpy_x - means_x)/deviations_x\n",
        "#test_numpy_y = (test_numpy_y - means_y)/deviations_y\n",
        "#print((test_numpy_x))\n",
        "\n",
        "dataset = TestDataset(test_numpy_x, test_numpy_y)"
      ],
      "metadata": {
        "id": "BjVdsoryf0-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = MLP()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.SmoothL1Loss()\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=2e-3)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(42)\n",
        "criterion = nn.SmoothL1Loss()"
      ],
      "metadata": {
        "id": "nzsNBHOjf2ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler, SequentialSampler \n",
        "\n",
        "#train_split = 0.2\n",
        "#random_seed = 42\n",
        "\n",
        "\n",
        "\n",
        "dataset_size = len(dataset)\n",
        "validation_split = 0\n",
        "validation_split_upper = 0.8\n",
        "#random_seed= 42\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "split_upper = int(np.floor(validation_split_upper * dataset_size))\n",
        "#train_indices, val_indices = indices[:split]+indices[split_upper:], indices[split:split_upper]\n",
        "val_indices, train_indices = indices[:split]+indices[split_upper:], indices[split:split_upper]\n",
        "\n",
        "#print(train_indices)\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "#train_sampler = Subset(dataset, train_indices)\n",
        "valid_dataset = Subset(dataset, val_indices)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=False, sampler=train_sampler, num_workers = 0)\n",
        "#train_loader = torch.utils.data.DataLoader(train_sampler, batch_size=64, shuffle=True, num_workers = 0)\n",
        "validation_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=64, shuffle=False, num_workers = 0)"
      ],
      "metadata": {
        "id": "Ooe1Sg_Lf3ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def valid_epoch(model,device,dataloader,loss_fn):\n",
        "    valid_loss, val_correct = 0.0, 0\n",
        "    r2_test = 0.0\n",
        "    RSS = 0\n",
        "    TSS = 0\n",
        "    avg = np.mean(test_numpy_y)\n",
        "    #s = open(r\"C:\\Users\\haavamfa\\Documents\\PhD\\test_results\\024_to_024_4layers_outlier_coords.txt\", \"w\")\n",
        "    #s.write(\"x y z\\n\")\n",
        "    f = open(\"output_comparison_foo.txt\", \"w\")\n",
        "    f = open(r\"C:\\Users\\haavamfa\\Documents\\PhD\\test_results\\2023\\march_speed\\output_foo.txt\", \"w\")\n",
        "    MAPE = 0.0\n",
        "    MAE = 0.0\n",
        "    model.eval()\n",
        "    f.write(\"Real Predicted\\n\")\n",
        "    for x, y in dataloader:\n",
        "\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        #print('Old:\\n')\n",
        "        #print(y)\n",
        "        y = y.view(-1, 1)\n",
        "        #print('New:\\n')\n",
        "        #print(y)\n",
        "        output = model(x.float())\n",
        "        #y = minMaxInverse(y, 20, 1390)\n",
        "        #output = (output * deviations_y) + means_y\n",
        "        loss=loss_fn(output,y)\n",
        "        valid_loss+=loss.item()*x.size(0)\n",
        "        scores, predictions = torch.max(output.data,1)\n",
        "        val_correct+=(abs(output-y) < y/20).sum().item()\n",
        "        output = output.detach().numpy()\n",
        "        y = y.detach().numpy()\n",
        "        for j in range(len(output)):\n",
        "          MAPE += abs((y[j]-output[j])/y[j])\n",
        "          MAE += abs(y[j]-output[j])\n",
        "          RSS += (y[j]-output[j])**2\n",
        "          TSS += (y[j] - avg)**2\n",
        "          #if abs(y[j]-output[j]) > 100:\n",
        "            #s.write\n",
        "          f.write(\"%4f %4f\\n\" % (y[j], output[j]))\n",
        "        #r2_test = R_squared(y, output)\n",
        "        #r2_test = R_squared_adj(r2_test, y.size, 8)\n",
        "    f.close()\n",
        "    R2 = 1 - (RSS/TSS)\n",
        "    R2adj = 1 - ((1 - R2)*(len(test_numpy_y)-1)/(len(test_numpy_y)-7))\n",
        "    return valid_loss,val_correct, MAPE, MAE, R2, R2adj"
      ],
      "metadata": {
        "id": "k-tjt2aMgUE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.use_deterministic_algorithms(True)\n",
        "mlp.train()\n",
        "  # Run the training loop\n",
        "for epoch in range(0, 5): # 5 epochs at maximum\n",
        "    \n",
        "    # Print epoch\n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    \n",
        "    # Set current loss value\n",
        "    current_loss = 0.0\n",
        "    epoch_loss = 0.0\n",
        "    count = 0\n",
        "    MAPE = 0\n",
        "    MAE = 0\n",
        "    \n",
        "    # Iterate over the DataLoader for training data\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "      \n",
        "      count += 1\n",
        "\n",
        "      # Get and prepare inputs\n",
        "      inputs, targets = data\n",
        "      inputs, targets = inputs.float(), targets.float()\n",
        "      targets = targets.reshape((targets.shape[0], 1))\n",
        "      \n",
        "      # Zero the gradients\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      # Perform forward pass\n",
        "      outputs = mlp(inputs)\n",
        "\n",
        "      #if epoch == 4:  \n",
        "      #  print('output is:\\n')\n",
        "      #  print('target is:\\n')\n",
        "      #print(targets)\n",
        "\n",
        "      # Compute loss\n",
        "      #targets = minMaxInverse(targets, 20, 1390)\n",
        "      #targets = (targets * deviations_y) + means_y\n",
        "      #outputs = (outputs * deviations_y) + means_y\n",
        "      loss = loss_function(outputs, targets)\n",
        "      for j in range(len(outputs)):\n",
        "        MAPE += abs((targets[j]-outputs[j])/targets[j])\n",
        "        MAE += abs(targets[j]-outputs[j])\n",
        "      # Perform backward pass\n",
        "      loss.backward()\n",
        "      \n",
        "      # Perform optimization\n",
        "      optimizer.step()\n",
        "      \n",
        "      # Print statistics\n",
        "      current_loss += loss.item()\n",
        "      epoch_loss += loss.item()\n",
        "      if i % 100 == 99:\n",
        "          print('Loss after mini-batch %5d: %.3f' %\n",
        "                (i + 1, current_loss / 100))\n",
        "          current_loss = 0.0\n",
        "    epoch_loss = epoch_loss/count\n",
        "    MAPE = MAPE*100/len(train_loader.sampler)\n",
        "    MAE = MAE/len(train_loader.sampler)\n",
        "    print('Smooth loss for this epoch is: %5f\\n' % epoch_loss)\n",
        "    print('MAE for this epoch is: %5f\\n' % MAE)\n",
        "    print('MAPE for this epoch is: %5f\\n' % MAPE)\n",
        "  # Process is complete.\n",
        "\n",
        "print('Training process has finished.')"
      ],
      "metadata": {
        "id": "GOh_EeE7NyFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(mlp.state_dict(), r'C:\\Users\\haavamfa\\Documents\\PhD\\Abaqus_Preprocessed\\jan_mlp_trained_048_4layers_0,0125speed_0.8_madehere_shuffled.pt')"
      ],
      "metadata": {
        "id": "v3jYgBFUIp-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = MLP()\n",
        "mlp.load_state_dict(torch.load(r'C:\\Users\\haavamfa\\Documents\\PhD\\Abaqus_Preprocessed\\jan_mlp_trained_048_6layers_0.8_seed59.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCuo15Vb65mq",
        "outputId": "2dff0b14-003d-49cc-ffbe-cd8c9af2f4b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Z87e_VkLA-6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Validation alt.\n",
        "\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "test_loss, test_correct, test_MAPE, test_MAE, test_R2, test_R2adj =valid_epoch(mlp,device,validation_loader,loss_function)\n",
        "\n",
        "\n",
        "test_loss = test_loss / len(validation_loader.sampler)\n",
        "test_correct = test_correct / len(validation_loader.sampler) * 100\n",
        "test_MAPE = test_MAPE * 100 / len(validation_loader.sampler)\n",
        "test_MAE = test_MAE/len(validation_loader.sampler)\n",
        "\n",
        "print('Test loss: %5f Test correct: %5f Test MAPE: %5f Test MAE: %5f R2: %5f Adjusted R2: %5f'%(test_loss, test_correct, test_MAPE, test_MAE, test_R2, test_R2adj))"
      ],
      "metadata": {
        "id": "cviWW3vQgXYX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcc428f4-ad8b-4d6d-ff7c-4680bed978b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.195189 Test correct: 99.921626 Test MAPE: 0.106808 Test MAE: 0.421974 R2: 0.998413 Adjusted R2: 0.998413\n"
          ]
        }
      ]
    }
  ]
}